{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a7299841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8660be28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"heart.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b35ee6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the relevant features for K-means (excluding \"Time\" and \"Class\")\n",
    "X = df.iloc[:, 0:13]  # Select columns V1 to V28 and Amount\n",
    "y = df.iloc[:,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "970eaab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "417642c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the feature values (optional but often helpful)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "314e7f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the VAE architecture\n",
    "original_dim = X_train.shape[1]\n",
    "latent_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f27afc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "encoder_inputs = keras.layers.Input(shape=(original_dim,))\n",
    "encoder_layer = keras.layers.Dense(128, activation='relu')(encoder_inputs)\n",
    "z_mean = keras.layers.Dense(latent_dim)(encoder_layer)\n",
    "z_log_var = keras.layers.Dense(latent_dim)(encoder_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b97d31e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling layer\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = tf.keras.backend.random_normal(shape=(tf.shape(z_mean)[0], latent_dim))\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "z = keras.layers.Lambda(sampling)([z_mean, z_log_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8629caeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "decoder_inputs = keras.layers.Input(shape=(latent_dim,))\n",
    "decoder_layer = keras.layers.Dense(128, activation='relu')(decoder_inputs)\n",
    "outputs = keras.layers.Dense(original_dim, activation='sigmoid')(decoder_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5f9519f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the VAE model\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "decoder = keras.Model(decoder_inputs, outputs, name='decoder')\n",
    "vae_outputs = decoder(encoder(encoder_inputs)[2])\n",
    "vae = keras.Model(encoder_inputs, vae_outputs, name='vae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "674f663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the VAE loss\n",
    "reconstruction_loss = tf.keras.losses.mean_squared_error(encoder_inputs, vae_outputs)\n",
    "kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "vae_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "vae.add_loss(vae_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e1ee00a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "21/21 [==============================] - 1s 10ms/step - loss: 0.9977 - val_loss: 1.0112\n",
      "Epoch 2/10\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 0.9977 - val_loss: 1.0110\n",
      "Epoch 3/10\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.9976 - val_loss: 1.0115\n",
      "Epoch 4/10\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.9977 - val_loss: 1.0108\n",
      "Epoch 5/10\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.9979 - val_loss: 1.0114\n",
      "Epoch 6/10\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 0.9978 - val_loss: 1.0110\n",
      "Epoch 7/10\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 0.9977 - val_loss: 1.0115\n",
      "Epoch 8/10\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 0.9977 - val_loss: 1.0112\n",
      "Epoch 9/10\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.9977 - val_loss: 1.0114\n",
      "Epoch 10/10\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.9976 - val_loss: 1.0114\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26fee7061f0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.compile(optimizer='adam')\n",
    "\n",
    "# Train the VAE model\n",
    "vae.fit(X_train, epochs=10, batch_size=32, validation_split=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a87b0547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "Epoch 1/10\n",
      "21/21 [==============================] - 1s 11ms/step - loss: 0.6928 - accuracy: 0.4985 - val_loss: 0.6917 - val_accuracy: 0.5244\n",
      "Epoch 2/10\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.6920 - accuracy: 0.5137 - val_loss: 0.6917 - val_accuracy: 0.5244\n",
      "Epoch 3/10\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.6918 - accuracy: 0.5137 - val_loss: 0.6914 - val_accuracy: 0.5244\n",
      "Epoch 4/10\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.6916 - accuracy: 0.5274 - val_loss: 0.6914 - val_accuracy: 0.5305\n",
      "Epoch 5/10\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.6914 - accuracy: 0.5168 - val_loss: 0.6910 - val_accuracy: 0.5244\n",
      "Epoch 6/10\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.6910 - accuracy: 0.5137 - val_loss: 0.6910 - val_accuracy: 0.5244\n",
      "Epoch 7/10\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.6909 - accuracy: 0.5137 - val_loss: 0.6908 - val_accuracy: 0.5244\n",
      "Epoch 8/10\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.6906 - accuracy: 0.5305 - val_loss: 0.6907 - val_accuracy: 0.5305\n",
      "Epoch 9/10\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.6904 - accuracy: 0.5427 - val_loss: 0.6906 - val_accuracy: 0.5305\n",
      "Epoch 10/10\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.6902 - accuracy: 0.5198 - val_loss: 0.6904 - val_accuracy: 0.5305\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6935 - accuracy: 0.5220\n",
      "Test accuracy: 0.5219511985778809\n"
     ]
    }
   ],
   "source": [
    "# Encode the data\n",
    "encoded_X_train, _, _ = encoder.predict(X_train)\n",
    "encoded_X_test, _, _ = encoder.predict(X_test)\n",
    "\n",
    "# Build a disease prediction model using the encoded features\n",
    "disease_model = keras.Sequential([\n",
    "    keras.layers.Input(shape=(latent_dim,)),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')  # Use 'sigmoid' for binary classification\n",
    "])\n",
    "\n",
    "disease_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the disease prediction model\n",
    "disease_model.fit(encoded_X_train, y_train, epochs=10, batch_size=32, validation_split=0.20)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = disease_model.evaluate(encoded_X_test, y_test)\n",
    "print(\"Test accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "051e61d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "26/26 [==============================] - 1s 7ms/step - loss: 1.2805 - val_loss: 1.3272\n",
      "Epoch 2/20\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1.2404 - val_loss: 1.2862\n",
      "Epoch 3/20\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1.2025 - val_loss: 1.2475\n",
      "Epoch 4/20\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1.1664 - val_loss: 1.2096\n",
      "Epoch 5/20\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 1.1307 - val_loss: 1.1727\n",
      "Epoch 6/20\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 1.0950 - val_loss: 1.1358\n",
      "Epoch 7/20\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 1.0592 - val_loss: 1.0997\n",
      "Epoch 8/20\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1.0242 - val_loss: 1.0651\n",
      "Epoch 9/20\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.9901 - val_loss: 1.0321\n",
      "Epoch 10/20\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.9583 - val_loss: 1.0018\n",
      "Epoch 11/20\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.9298 - val_loss: 0.9747\n",
      "Epoch 12/20\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.9049 - val_loss: 0.9507\n",
      "Epoch 13/20\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.8834 - val_loss: 0.9302\n",
      "Epoch 14/20\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.8645 - val_loss: 0.9124\n",
      "Epoch 15/20\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.8480 - val_loss: 0.8968\n",
      "Epoch 16/20\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.8334 - val_loss: 0.8828\n",
      "Epoch 17/20\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.8200 - val_loss: 0.8704\n",
      "Epoch 18/20\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.8079 - val_loss: 0.8592\n",
      "Epoch 19/20\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.7969 - val_loss: 0.8491\n",
      "Epoch 20/20\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.7868 - val_loss: 0.8398\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.8398\n",
      "Test loss: 0.839785099029541\n",
      "7/7 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"heart.csv\")\n",
    "\n",
    "# Split the data into features and target\n",
    "X = data.drop(\"target\", axis=1)\n",
    "y = data[\"target\"]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the autoencoder model\n",
    "input_dim = X_train.shape[1]\n",
    "encoding_dim = 10  # Adjust the latent space dimension as needed\n",
    "\n",
    "autoencoder = keras.Sequential([\n",
    "    layers.Input(shape=(input_dim,)),\n",
    "    layers.Dense(encoding_dim, activation='relu'),\n",
    "    layers.Dense(input_dim, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the autoencoder\n",
    "epochs = 20  # Adjust as needed\n",
    "batch_size = 32\n",
    "\n",
    "autoencoder.fit(X_train, X_train, epochs=epochs, batch_size=batch_size, shuffle=True, validation_data=(X_test, X_test))\n",
    "\n",
    "# Evaluate the autoencoder\n",
    "test_loss = autoencoder.evaluate(X_test, X_test)\n",
    "print(f\"Test loss: {test_loss}\")\n",
    "\n",
    "# Use the trained autoencoder for reconstruction\n",
    "reconstructed_data = autoencoder.predict(X_test)\n",
    "\n",
    "# You can calculate accuracy or other metrics for the reconstruction\n",
    "# Compare X_test and reconstructed_data to assess the autoencoder's performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67397d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
